# System Architecture

## Application Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INTERFACE                           â”‚
â”‚                   (Streamlit App)                            â”‚
â”‚                                                              â”‚
â”‚  [Ticker Input] [Source Selection] [Analysis Mode] [Cache]  â”‚
â”‚                                                              â”‚
â”‚              [ğŸš€ Analyze Sentiment Button]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MAIN APPLICATION                           â”‚
â”‚              (stock_sentiment_app.py)                        â”‚
â”‚                                                              â”‚
â”‚  â€¢ Parse & validate tickers                                  â”‚
â”‚  â€¢ Check cache for existing data                             â”‚
â”‚  â€¢ Manage progress tracking                                  â”‚
â”‚  â€¢ Coordinate parallel processing                            â”‚
â”‚  â€¢ Aggregate results                                         â”‚
â”‚  â€¢ Update session state                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                         â”‚
             â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NEWS SCRAPING        â”‚   â”‚   SENTIMENT ANALYSIS           â”‚
â”‚  (news_scrapers.py)    â”‚   â”‚  (sentiment_analyzer.py)       â”‚
â”‚                        â”‚   â”‚                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  scrape_finviz()   â”‚ â”‚   â”‚ â”‚  analyze_vader_sentiment() â”‚ â”‚
â”‚ â”‚  â€¢ Parse HTML      â”‚ â”‚   â”‚ â”‚  â€¢ Rule-based analysis     â”‚ â”‚
â”‚ â”‚  â€¢ Extract news    â”‚ â”‚   â”‚ â”‚  â€¢ Compound score          â”‚ â”‚
â”‚ â”‚  â€¢ Get headlines   â”‚ â”‚   â”‚ â”‚  â€¢ Fast execution          â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚   â”‚                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚  scrape_yahoo()    â”‚ â”‚   â”‚ â”‚ analyze_finbert_sentiment()â”‚ â”‚
â”‚ â”‚  â€¢ Parse HTML      â”‚ â”‚   â”‚ â”‚  â€¢ ML-based analysis       â”‚ â”‚
â”‚ â”‚  â€¢ Extract news    â”‚ â”‚   â”‚ â”‚  â€¢ Confidence scores       â”‚ â”‚
â”‚ â”‚  â€¢ Get content     â”‚ â”‚   â”‚ â”‚  â€¢ Financial context       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚   â”‚                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ scrape_google_news()â”‚ â”‚   â”‚ â”‚  load_finbert()           â”‚ â”‚
â”‚ â”‚  â€¢ Parse HTML      â”‚ â”‚   â”‚ â”‚  â€¢ Lazy model loading      â”‚ â”‚
â”‚ â”‚  â€¢ Extract news    â”‚ â”‚   â”‚ â”‚  â€¢ Tokenization            â”‚ â”‚
â”‚ â”‚  â€¢ Get headlines   â”‚ â”‚   â”‚ â”‚  â€¢ Batch processing        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚   â”‚                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”‚fetch_article_      â”‚ â”‚
â”‚ â”‚content()           â”‚ â”‚
â”‚ â”‚  â€¢ HTTP requests   â”‚ â”‚
â”‚ â”‚  â€¢ Content extract â”‚ â”‚
â”‚ â”‚  â€¢ Text cleaning   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   UTILITY FUNCTIONS                          â”‚
â”‚                     (utils.py)                               â”‚
â”‚                                                              â”‚
â”‚  â€¢ validate_ticker() - Check ticker validity                 â”‚
â”‚  â€¢ clean_ticker() - Format ticker symbols                    â”‚
â”‚  â€¢ format_results() - Structure output data                  â”‚
â”‚  â€¢ calculate_sentiment_ratio() - P/N ratio                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

```
Input Tickers (e.g., "AAPL, MSFT, GOOGL")
    â”‚
    â–¼
Parse & Validate
    â”‚
    â”œâ”€â†’ AAPL â”€â”€â”¬â”€â†’ Check Cache â”€â”€â”¬â”€â†’ [HIT] â†’ Use cached data
    â”‚          â”‚                 â”‚
    â”‚          â””â”€â†’ [MISS] â”€â”€â”¬â”€â”€â”€â”€â”´â”€â†’ scrape_finviz()
    â”‚                        â”œâ”€â”€â”€â”€â”€â”€â†’ scrape_yahoo()
    â”‚                        â””â”€â”€â”€â”€â”€â”€â†’ scrape_google_news()
    â”‚                                     â”‚
    â”‚                                     â–¼
    â”‚                           Collect News Articles
    â”‚                           (Headlines + Content)
    â”‚                                     â”‚
    â”‚                                     â–¼
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> For Each Article:
    â”‚                                     â”‚
    â”‚                                     â”œâ”€â†’ analyze_vader_sentiment()
    â”‚                                     â”‚   â€¢ Get label (pos/neg/neu)
    â”‚                                     â”‚   â€¢ Get compound score
    â”‚                                     â”‚
    â”‚                                     â””â”€â†’ analyze_finbert_sentiment()
    â”‚                                         â€¢ Get label (pos/neg/neu)
    â”‚                                         â€¢ Get confidence score
    â”‚                                                 â”‚
    â”‚                                                 â–¼
    â”‚                                      Store Results with:
    â”‚                                      â€¢ Ticker
    â”‚                                      â€¢ Source
    â”‚                                      â€¢ Headline
    â”‚                                      â€¢ Date
    â”‚                                      â€¢ VADER sentiment & score
    â”‚                                      â€¢ FinBERT sentiment & score
    â”‚                                      â€¢ URL
    â”‚
    â”œâ”€â†’ MSFT (same process)
    â”‚
    â””â”€â†’ GOOGL (same process)
            â”‚
            â–¼
    Aggregate All Results
            â”‚
            â”œâ”€â†’ Create Summary Table:
            â”‚   â€¢ Count positive/negative/neutral per ticker
            â”‚   â€¢ Calculate sentiment ratios
            â”‚   â€¢ Sort by ratio
            â”‚
            â””â”€â†’ Create Detailed Table:
                â€¢ All articles with full sentiment data
                â€¢ Filterable by ticker
                â”‚
                â–¼
    Display Results + Export Options
```

## Component Interaction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit App   â”‚
â”‚  (Frontend)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ User input, configuration
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Controller â”‚â—„â”€â”€â”€â”€â”€â”€â”
â”‚  (Orchestration) â”‚       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
         â”‚                 â”‚
         â”‚ Parallel        â”‚ Cache
         â”‚ Processing      â”‚ Read/Write
         â”‚                 â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Scraper â”‚      â”‚  Cache  â”‚
    â”‚ Module  â”‚      â”‚ Storage â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Raw news data
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Sentiment  â”‚
    â”‚ Analyzer   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Analyzed data
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Results   â”‚
    â”‚ Aggregator â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Formatted results
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Display Layer   â”‚
â”‚  (Tables, CSV)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Caching Strategy

```
Request for Ticker "AAPL" at 10:00 AM
    â”‚
    â–¼
Generate Cache Key: "AAPL_Finviz_202511081000"
    â”‚
    â”œâ”€â†’ Check st.session_state.cache
    â”‚
    â”œâ”€â†’ [KEY EXISTS & FRESH]
    â”‚   â”‚
    â”‚   â””â”€â†’ Return cached data (instant)
    â”‚
    â””â”€â†’ [KEY MISSING OR EXPIRED]
        â”‚
        â””â”€â†’ Scrape fresh data
            â”‚
            â””â”€â†’ Store in cache with timestamp
                â”‚
                â””â”€â†’ Return fresh data
```

## Error Handling Flow

```
Operation Attempt
    â”‚
    â”œâ”€â†’ [SUCCESS] â†’ Continue
    â”‚
    â””â”€â†’ [ERROR]
        â”‚
        â”œâ”€â†’ Network Error
        â”‚   â””â”€â†’ Log warning
        â”‚       â””â”€â†’ Continue with other sources
        â”‚
        â”œâ”€â†’ Parsing Error
        â”‚   â””â”€â†’ Skip article
        â”‚       â””â”€â†’ Continue with next article
        â”‚
        â”œâ”€â†’ Invalid Ticker
        â”‚   â””â”€â†’ Skip ticker
        â”‚       â””â”€â†’ Continue with next ticker
        â”‚
        â””â”€â†’ Model Error
            â””â”€â†’ Fallback to default sentiment
                â””â”€â†’ Continue processing
```

## Performance Optimization

### Parallel Processing
```
Sequential (slow):
AAPL â†’ wait â†’ MSFT â†’ wait â†’ GOOGL â†’ wait (90 seconds)

Parallel (fast):
â”Œâ”€â†’ AAPL â”€â”
â”œâ”€â†’ MSFT â”€â”¤â†’ All complete (30 seconds)
â””â”€â†’ GOOGL â”€â”˜
```

### Lazy Loading
```
App Start â†’ Load UI (instant)
    â”‚
    â””â”€â†’ First FinBERT request â†’ Load model (2 min)
        â”‚
        â””â”€â†’ Subsequent requests â†’ Use cached model (instant)
```

### Caching Benefits
```
Without Cache:
Request 1 â†’ Scrape (5s) â†’ Analyze (3s) = 8s
Request 2 â†’ Scrape (5s) â†’ Analyze (3s) = 8s
Total: 16s

With Cache:
Request 1 â†’ Scrape (5s) â†’ Analyze (3s) â†’ Cache = 8s
Request 2 â†’ Cache hit (0.1s) â†’ Analyze (3s) = 3.1s
Total: 11.1s (31% faster)
```

## Technology Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend Layer              â”‚
â”‚  â€¢ Streamlit (UI framework)         â”‚
â”‚  â€¢ HTML/CSS (via Streamlit)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Application Layer             â”‚
â”‚  â€¢ Python 3.8+                      â”‚
â”‚  â€¢ Pandas (data manipulation)       â”‚
â”‚  â€¢ Session state management         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Data Layer                  â”‚
â”‚  â€¢ Requests (HTTP)                  â”‚
â”‚  â€¢ BeautifulSoup4 (parsing)         â”‚
â”‚  â€¢ In-memory caching                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AI/ML Layer                â”‚
â”‚  â€¢ VADER (rule-based NLP)           â”‚
â”‚  â€¢ FinBERT (transformer model)      â”‚
â”‚  â€¢ PyTorch (deep learning)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Scalability Considerations

### Current Limits
- **Tickers:** 30 per request
- **Articles:** 5 per ticker per source
- **Max Articles:** 450 total (30 Ã— 5 Ã— 3)
- **Processing Time:** 2-5 minutes for max load

### Scaling Options

1. **Horizontal Scaling**
   - Deploy multiple instances
   - Load balance requests
   - Shared cache layer (Redis)

2. **Vertical Scaling**
   - Increase FinBERT batch size
   - Add more CPU cores
   - Increase memory for larger models

3. **Optimization**
   - Implement database caching
   - Pre-compute popular tickers
   - Add CDN for static assets
   - Use async scraping

---

This architecture provides a solid foundation for efficient, scalable sentiment analysis while maintaining code modularity and maintainability.
